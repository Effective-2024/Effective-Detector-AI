{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-17 10:42:01.254908: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-04-17 10:42:01.254950: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-04-17 10:42:01.256516: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-04-17 10:42:01.264690: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-17 10:42:03.561526: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "import pathlib\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-17 10:43:08.381379: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 10230 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1080 Ti, pci bus id: 0000:03:00.0, compute capability: 6.1\n",
      "2024-04-17 10:43:08.382161: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 10486 MB memory:  -> device: 1, name: NVIDIA GeForce GTX 1080 Ti, pci bus id: 0000:04:00.0, compute capability: 6.1\n",
      "2024-04-17 10:43:08.382901: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 10527 MB memory:  -> device: 2, name: NVIDIA GeForce GTX 1080 Ti, pci bus id: 0000:81:00.0, compute capability: 6.1\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.load_model('./first_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tflite_models_dir = pathlib.Path(\"./tflite_models\")\n",
    "tflite_model_file = tflite_models_dir/\"first_model.tflite\"\n",
    "tflite_model_quant_file = tflite_models_dir/\"first_model_quant.tflite\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmppezt2hv1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmppezt2hv1/assets\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m converter \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mlite\u001b[38;5;241m.\u001b[39mTFLiteConverter\u001b[38;5;241m.\u001b[39mfrom_keras_model(model)\n\u001b[0;32m----> 2\u001b[0m tflite_model \u001b[38;5;241m=\u001b[39m \u001b[43mconverter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m tflite_models_dir \u001b[38;5;241m=\u001b[39m pathlib\u001b[38;5;241m.\u001b[39mPath(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./tflite_models\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m tflite_models_dir\u001b[38;5;241m.\u001b[39mmkdir(exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, parents\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/media/cis/새 볼륨/audio/myenv/lib/python3.11/site-packages/tensorflow/lite/python/lite.py:1139\u001b[0m, in \u001b[0;36m_export_metrics.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1136\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(convert_func)\n\u001b[1;32m   1137\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1138\u001b[0m   \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m-> 1139\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_and_export_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/media/cis/새 볼륨/audio/myenv/lib/python3.11/site-packages/tensorflow/lite/python/lite.py:1093\u001b[0m, in \u001b[0;36mTFLiteConverterBase._convert_and_export_metrics\u001b[0;34m(self, convert_func, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1091\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save_conversion_params_metric()\n\u001b[1;32m   1092\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mprocess_time()\n\u001b[0;32m-> 1093\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1094\u001b[0m elapsed_time_ms \u001b[38;5;241m=\u001b[39m (time\u001b[38;5;241m.\u001b[39mprocess_time() \u001b[38;5;241m-\u001b[39m start_time) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1000\u001b[39m\n\u001b[1;32m   1095\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result:\n",
      "File \u001b[0;32m/media/cis/새 볼륨/audio/myenv/lib/python3.11/site-packages/tensorflow/lite/python/lite.py:1601\u001b[0m, in \u001b[0;36mTFLiteKerasModelConverterV2.convert\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1588\u001b[0m \u001b[38;5;129m@_export_metrics\u001b[39m\n\u001b[1;32m   1589\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconvert\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   1590\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Converts a keras model based on instance variables.\u001b[39;00m\n\u001b[1;32m   1591\u001b[0m \n\u001b[1;32m   1592\u001b[0m \u001b[38;5;124;03m  Returns:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1599\u001b[0m \u001b[38;5;124;03m      Invalid quantization parameters.\u001b[39;00m\n\u001b[1;32m   1600\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1601\u001b[0m   saved_model_convert_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_as_saved_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1602\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m saved_model_convert_result:\n\u001b[1;32m   1603\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m saved_model_convert_result\n",
      "File \u001b[0;32m/media/cis/새 볼륨/audio/myenv/lib/python3.11/site-packages/tensorflow/lite/python/lite.py:1579\u001b[0m, in \u001b[0;36mTFLiteKerasModelConverterV2._convert_as_saved_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1576\u001b[0m temp_dir \u001b[38;5;241m=\u001b[39m tempfile\u001b[38;5;241m.\u001b[39mmkdtemp()\n\u001b[1;32m   1577\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1578\u001b[0m   graph_def, input_tensors, output_tensors \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m-> 1579\u001b[0m       \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_keras_to_saved_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtemp_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1580\u001b[0m   )\n\u001b[1;32m   1581\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msaved_model_dir:\n\u001b[1;32m   1582\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m(TFLiteKerasModelConverterV2, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mconvert(\n\u001b[1;32m   1583\u001b[0m         graph_def, input_tensors, output_tensors\n\u001b[1;32m   1584\u001b[0m     )\n",
      "File \u001b[0;32m/media/cis/새 볼륨/audio/myenv/lib/python3.11/site-packages/tensorflow/lite/python/convert_phase.py:205\u001b[0m, in \u001b[0;36mconvert_phase.<locals>.actual_decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    204\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m ConverterError \u001b[38;5;28;01mas\u001b[39;00m converter_error:\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m converter_error\u001b[38;5;241m.\u001b[39merrors:\n",
      "File \u001b[0;32m/media/cis/새 볼륨/audio/myenv/lib/python3.11/site-packages/tensorflow/lite/python/lite.py:1502\u001b[0m, in \u001b[0;36mTFLiteKerasModelConverterV2._convert_keras_to_saved_model\u001b[0;34m(self, output_dir)\u001b[0m\n\u001b[1;32m   1491\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Save Keras model to the SavedModel format.\u001b[39;00m\n\u001b[1;32m   1492\u001b[0m \n\u001b[1;32m   1493\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1499\u001b[0m \u001b[38;5;124;03m  output_tensors: List of output tensors.\u001b[39;00m\n\u001b[1;32m   1500\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1501\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1502\u001b[0m   \u001b[43m_save\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1503\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_keras_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1504\u001b[0m \u001b[43m      \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1505\u001b[0m \u001b[43m      \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_save_options\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSaveOptions\u001b[49m\u001b[43m(\u001b[49m\u001b[43msave_debug_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1506\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1507\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m   1508\u001b[0m   \u001b[38;5;66;03m# When storing the given keras model to a saved model is failed, let's\u001b[39;00m\n\u001b[1;32m   1509\u001b[0m   \u001b[38;5;66;03m# use original keras model conversion pipeline.\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/media/cis/새 볼륨/audio/myenv/lib/python3.11/site-packages/tensorflow/python/saved_model/save.py:1336\u001b[0m, in \u001b[0;36msave\u001b[0;34m(obj, export_dir, signatures, options)\u001b[0m\n\u001b[1;32m   1334\u001b[0m \u001b[38;5;66;03m# pylint: enable=line-too-long\u001b[39;00m\n\u001b[1;32m   1335\u001b[0m metrics\u001b[38;5;241m.\u001b[39mIncrementWriteApi(_SAVE_V2_LABEL)\n\u001b[0;32m-> 1336\u001b[0m \u001b[43msave_and_return_nodes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexport_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msignatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1338\u001b[0m metrics\u001b[38;5;241m.\u001b[39mIncrementWrite(write_version\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/media/cis/새 볼륨/audio/myenv/lib/python3.11/site-packages/tensorflow/python/saved_model/save.py:1417\u001b[0m, in \u001b[0;36msave_and_return_nodes\u001b[0;34m(obj, export_dir, signatures, options, experimental_skip_checkpoint)\u001b[0m\n\u001b[1;32m   1415\u001b[0m \u001b[38;5;66;03m# Save debug info, if requested.\u001b[39;00m\n\u001b[1;32m   1416\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m options\u001b[38;5;241m.\u001b[39msave_debug_info:\n\u001b[0;32m-> 1417\u001b[0m   \u001b[43m_export_debug_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexported_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexport_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1418\u001b[0m \u001b[38;5;66;03m# For privacy concerns, please see the note in\u001b[39;00m\n\u001b[1;32m   1419\u001b[0m \u001b[38;5;66;03m#  tensorflow/cc/saved_model/metrics.h\u001b[39;00m\n\u001b[1;32m   1420\u001b[0m metrics\u001b[38;5;241m.\u001b[39mSetWritePath(saved_model_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(export_dir))\n",
      "File \u001b[0;32m/media/cis/새 볼륨/audio/myenv/lib/python3.11/site-packages/tensorflow/python/saved_model/save.py:1135\u001b[0m, in \u001b[0;36m_export_debug_info\u001b[0;34m(exported_graph, export_dir)\u001b[0m\n\u001b[1;32m   1133\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(fn, defun\u001b[38;5;241m.\u001b[39mAtomicFunction):  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m   1134\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m-> 1135\u001b[0m   debug_builder\u001b[38;5;241m.\u001b[39mAppendGraphDebugInfo(fn_name, \u001b[43mfn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgraph_debug_info\u001b[49m)\n\u001b[1;32m   1137\u001b[0m graph_debug_info \u001b[38;5;241m=\u001b[39m debug_builder\u001b[38;5;241m.\u001b[39mBuild()\n\u001b[1;32m   1138\u001b[0m file_io\u001b[38;5;241m.\u001b[39matomic_write_string_to_file(\n\u001b[1;32m   1139\u001b[0m     file_io\u001b[38;5;241m.\u001b[39mjoin(\n\u001b[1;32m   1140\u001b[0m         path_helpers\u001b[38;5;241m.\u001b[39mget_or_create_debug_dir(export_dir),\n\u001b[1;32m   1141\u001b[0m         constants\u001b[38;5;241m.\u001b[39mDEBUG_INFO_FILENAME_PB),\n\u001b[1;32m   1142\u001b[0m     graph_debug_info\u001b[38;5;241m.\u001b[39mSerializeToString(deterministic\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\n",
      "File \u001b[0;32m/media/cis/새 볼륨/audio/myenv/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:157\u001b[0m, in \u001b[0;36mAtomicFunction.graph_debug_info\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgraph_debug_info\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m graph_debug_info_pb2\u001b[38;5;241m.\u001b[39mGraphDebugInfo:\n\u001b[1;32m    156\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"A GraphDebugInfo proto mapping nodes to corresponding stack traces.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 157\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_graph_debug_info\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/media/cis/새 볼륨/audio/myenv/lib/python3.11/site-packages/tensorflow/python/eager/context.py:1415\u001b[0m, in \u001b[0;36mContext.get_graph_debug_info\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1403\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Get GraphDebugInfo associated with a function from the context.\u001b[39;00m\n\u001b[1;32m   1404\u001b[0m \n\u001b[1;32m   1405\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1412\u001b[0m \u001b[38;5;124;03m  tf.errors.NotFoundError: if name is not the name of a registered function.\u001b[39;00m\n\u001b[1;32m   1413\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1414\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m c_api_util\u001b[38;5;241m.\u001b[39mtf_buffer() \u001b[38;5;28;01mas\u001b[39;00m buffer_:\n\u001b[0;32m-> 1415\u001b[0m   \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_ContextGetGraphDebugInfo\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1416\u001b[0m   proto_data \u001b[38;5;241m=\u001b[39m pywrap_tf_session\u001b[38;5;241m.\u001b[39mTF_GetBuffer(buffer_)\n\u001b[1;32m   1417\u001b[0m graph_debug_info \u001b[38;5;241m=\u001b[39m graph_debug_info_pb2\u001b[38;5;241m.\u001b[39mGraphDebugInfo()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "\n",
    "tflite_models_dir = pathlib.Path(\"./tflite_models\")\n",
    "tflite_models_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "tflite_model_file = tflite_models_dir/\"first_model.tflite\"\n",
    "tflite_model_file.write_bytes(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp9loz5pxd/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp9loz5pxd/assets\n",
      "2024-04-17 10:04:24.980631: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:378] Ignored output_format.\n",
      "2024-04-17 10:04:24.980669: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:381] Ignored drop_control_dependency.\n",
      "2024-04-17 10:04:24.980921: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /tmp/tmp9loz5pxd\n",
      "2024-04-17 10:04:25.051447: I tensorflow/cc/saved_model/reader.cc:51] Reading meta graph with tags { serve }\n",
      "2024-04-17 10:04:25.051491: I tensorflow/cc/saved_model/reader.cc:146] Reading SavedModel debug info (if present) from: /tmp/tmp9loz5pxd\n",
      "2024-04-17 10:04:25.205378: I tensorflow/cc/saved_model/loader.cc:233] Restoring SavedModel bundle.\n",
      "2024-04-17 10:04:25.953337: I tensorflow/cc/saved_model/loader.cc:217] Running initialization op on SavedModel bundle at path: /tmp/tmp9loz5pxd\n",
      "2024-04-17 10:04:26.265049: I tensorflow/cc/saved_model/loader.cc:316] SavedModel load for tags { serve }; Status: success: OK. Took 1284129 microseconds.\n",
      "Summary on the non-converted ops:\n",
      "---------------------------------\n",
      " * Accepted dialects: tfl, builtin, func\n",
      " * Non-Converted Ops: 111, Total Ops 496, % non-converted = 22.38 %\n",
      " * 111 ARITH ops\n",
      "\n",
      "- arith.constant:  111 occurrences  (f32: 98, i32: 13)\n",
      "\n",
      "\n",
      "\n",
      "  (f32: 9)\n",
      "  (f32: 65)\n",
      "  (f32: 16)\n",
      "  (f32: 1)\n",
      "  (f32: 65)\n",
      "  (f32: 17)\n",
      "  (f32: 68)\n",
      "  (i32: 16)\n",
      "  (f32: 5)\n",
      "  (uq_8: 70)\n",
      "  (f32: 16)\n",
      "  (i32: 16)\n",
      "  (f32: 1)\n",
      "  (i32: 16)\n",
      "  (f32: 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4547632"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "tflite_model_quant = converter.convert()\n",
    "\n",
    "tflite_model_quant_file = tflite_models_dir/\"first_model_quant.tflite\"\n",
    "tflite_model_quant_file.write_bytes(tflite_model_quant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = (224, 224)\n",
    "BATCH_SIZE = 8\n",
    "test_dir = './converted_dataset/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 14564 files belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "global test_images, test_labels\n",
    "\n",
    "test_dataset = keras.utils.image_dataset_from_directory(\n",
    "    test_dir,\n",
    "    image_size = IMAGE_SIZE,\n",
    "    batch_size = BATCH_SIZE,\n",
    "    seed=2024\n",
    ")\n",
    "\n",
    "subset = test_dataset.take(10)\n",
    "\n",
    "test_images = []\n",
    "test_labels = []\n",
    "for images, labels in subset:\n",
    "    for image in images:\n",
    "        test_images.append(image)\n",
    "    for label in labels:\n",
    "        test_labels.append(label)\n",
    "test_images = np.array(test_images)\n",
    "test_labels = np.array(test_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to run inference on a TFLite model\n",
    "def run_tflite_model(tflite_file, test_image_indices):\n",
    "  global test_images\n",
    "\n",
    "  # Initialize the interpreter\n",
    "  interpreter = tf.lite.Interpreter(model_path=str(tflite_file))\n",
    "  interpreter.allocate_tensors()\n",
    "\n",
    "  input_details = interpreter.get_input_details()[0]\n",
    "  output_details = interpreter.get_output_details()[0]\n",
    "\n",
    "  predictions = np.zeros((len(test_image_indices),), dtype=int)\n",
    "  for i, test_image_index in enumerate(test_image_indices):\n",
    "    test_image = test_images[test_image_index]\n",
    "    test_label = test_labels[test_image_index]\n",
    "\n",
    "    # Check if the input type is quantized, then rescale input data to uint8\n",
    "    if input_details['dtype'] == np.uint8:\n",
    "      input_scale, input_zero_point = input_details[\"quantization\"]\n",
    "      test_image = test_image / input_scale + input_zero_point\n",
    "\n",
    "    test_image = np.expand_dims(test_image, axis=0).astype(input_details[\"dtype\"])\n",
    "    interpreter.set_tensor(input_details[\"index\"], test_image)\n",
    "    interpreter.invoke()\n",
    "    output = interpreter.get_tensor(output_details[\"index\"])[0]\n",
    "\n",
    "    predictions[i] = output.argmax()\n",
    "\n",
    "  return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to evaluate a TFLite model on all images\n",
    "def evaluate_model(tflite_file, model_type):\n",
    "  global test_images\n",
    "  global test_labels\n",
    "\n",
    "  test_image_indices = range(test_images.shape[0])\n",
    "  predictions = run_tflite_model(tflite_file, test_image_indices)\n",
    "\n",
    "  accuracy = (np.sum(test_labels== predictions) * 100) / len(test_images)\n",
    "\n",
    "  print('%s model accuracy is %.4f%% (Number of test samples=%d)' % (\n",
    "      model_type, accuracy, len(test_images)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Float model accuracy is 100.0000% (Number of test samples=80)\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(tflite_model_file, model_type=\"Float\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Float model accuracy is 100.0000% (Number of test samples=80)\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(tflite_model_quant_file, model_type=\"Float\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
